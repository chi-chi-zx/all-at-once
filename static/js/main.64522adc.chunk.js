(this["webpackJsonpall-at-once"]=this["webpackJsonpall-at-once"]||[]).push([[0],{18:function(e,t,n){},19:function(e,t,n){},22:function(e,t,n){"use strict";n.r(t);var a=n(0),i=n(1),s=n.n(i),r=n(8),o=n.n(r),c=(n(18),n(19),n(4)),l=n(5),d=n(12);function h(){var e=Object(c.a)(["\n  font-size: 24px;\n  color: #33353a;\n  line-height: 1.38;\n  margin: 0;\n  font-family: 'Maison Neue, sans-serif';\n  font-weight: bold\n"]);return h=function(){return e},e}function m(){var e=Object(c.a)(["\n  font-size: 32px;\n  color: #33353a;\n  line-height: 1.38;\n  margin: 0;\n  font-family: 'Maison Neue, sans-serif';\n  font-weight: bold\n"]);return m=function(){return e},e}function j(){var e=Object(c.a)(["\n  font-size: 16px;\n  color: #33353a;\n  line-height: 1.38;\n  margin: 0;\n  font-family: 'Maison Neue, sans-serif';\n  ",";\n"]);return j=function(){return e},e}var p=l.a.p(j(),d.a),u=l.a.p(m()),b=l.a.p(h()),f="All at Once: Temporally Adaptive Multi-Frame Interpolation with Advanced Motion Modeling",x=["Zhixiang Chi*","Rasoul Mohammadi Nasiri*","Zheng Liu*","Juwei Lu*","Jin Tang*","Konstantinos N Plataniotis#"],g=["*Noah\u2019s Ark Lab, Huawei Technologies","#University of Toronto, Canada"],v="ECCV2020",O="Abstract",w="Recent advances in high refresh rate displays as well as the increased interest in high rate of slow motion and frame up-conversion fuel the demand for efficient and cost-effective multi-frame video interpolation solutions.  To that regard, inserting multiple frames between consecutive video frames are of paramount importance for the consumer electronics industry. State-of-the-art methods are iterative solutions interpolating one frame at the time. They introduce temporal inconsistencies and clearly noticeable visual artifacts.",y="Departing from the state-of-the-art, this work introduces a true multi-frame interpolator. It utilizes a pyramidal style network in the temporal domain to complete the multi-frame interpolation task in one-shot. A novel flow estimation procedure using a relaxed loss function, and an advanced, cubic-based, motion model is also used to further boost interpolation accuracy when complex motion segments are encountered. Results on the Adobe240 dataset show that the proposed method generates visually pleasing, temporally consistent frames, outperforms the current best off-the-shelf method by 1.57db in PSNR with 8 times smaller model and 7.7 times faster. The proposed method can be easily extended to interpolate a large number of new frames while remaining efficient because of the one-shot mechanism.",N="Video Display",C="https://www.youtube.com/embed/2s6pWOR1dHk?autoplay=1&mute=1",k="Citation",M="Paper",A="https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123720103.pdf",T="https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123720103-supp.zip",I="Method overview",R="Results",L=n.p+"static/media/overview.f08960de.png",_=n.p+"static/media/compare.8d30a0c1.png",E=n.p+"static/media/sample.f1ee726a.png",F=n.p+"static/media/eff.7e151825.png",J=n.p+"static/media/pdfLogo.3dfcb5cd.png",P=function(){var e=f,t=x,n=g,i=v,s=O,r=w,o=y,c=N,l=C,d=k,h=M,m=A,j=T,P=I,S=R;return Object(a.jsxs)("div",{className:"page-container",children:[Object(a.jsxs)("div",{children:[Object(a.jsx)(u,{children:e}),Object(a.jsx)("div",{className:"text-row",children:t.map((function(e){return Object(a.jsx)(p,{style:{marginRight:"8px"},children:e+","},e)}))}),Object(a.jsx)("div",{className:"text-row",children:n.map((function(e){return Object(a.jsx)(p,{style:{marginRight:"8px"},children:e},e)}))}),Object(a.jsx)(p,{children:i})]}),Object(a.jsxs)("div",{className:"block-container",children:[Object(a.jsx)("div",{className:"text-row",style:{justifyContent:"start"},children:Object(a.jsx)(b,{children:s})}),Object(a.jsx)("div",{className:"content-text",children:r}),Object(a.jsx)("div",{className:"content-text",style:{marginTop:"16px"},children:o})]}),Object(a.jsxs)("div",{className:"block-container",children:[Object(a.jsx)("div",{className:"text-row",style:{justifyContent:"start"},children:Object(a.jsx)(b,{children:c})}),Object(a.jsx)("div",{className:"text-row",children:Object(a.jsx)("embed",{title:"all-at-once",width:"800",height:"550",src:l})})]}),Object(a.jsxs)("div",{className:"block-container",children:[Object(a.jsx)("div",{className:"text-row",style:{justifyContent:"start"},children:Object(a.jsx)(b,{children:h})}),Object(a.jsxs)("div",{className:"text-row",style:{justifyContent:"center"},children:[Object(a.jsxs)("div",{children:[Object(a.jsx)("a",{href:m,target:"_blank",rel:"noreferrer",children:Object(a.jsx)("img",{className:"paper-download",src:J,alt:"paper_download"})}),Object(a.jsx)(p,{children:"ECCV20"})]}),Object(a.jsxs)("div",{children:[Object(a.jsx)("a",{href:j,target:"_blank",rel:"noreferrer",children:Object(a.jsx)("img",{className:"paper-download",src:J,alt:"paper_download"})}),Object(a.jsx)(p,{children:"Supplementary"})]})]})]}),Object(a.jsxs)("div",{className:"block-container",children:[Object(a.jsx)("div",{className:"text-row",style:{justifyContent:"start"},children:Object(a.jsx)(b,{children:d})}),Object(a.jsx)("div",{style:{textAlign:"initial",backgroundColor:"#e9e9e9",padding:"16px"},children:Object(a.jsx)("code",{style:{whiteSpace:"pre-wrap"},children:"@inproceedings{chiall,\n  author = {Zhixiang Chi, Rasoul Mohammadi Nasiri, Zheng Liu, Juwei Lu, Jin Tang, Konstantinos N Plataniotis}, \n  title  = {All at Once: Temporally Adaptive Multi-Frame Interpolation with Advanced Motion Modeling}, \n  booktitle = {European Conference on Computer Vision},\n  year   = {2020}\n}"})})]}),Object(a.jsxs)("div",{className:"block-container",children:[Object(a.jsx)("div",{className:"text-row",style:{justifyContent:"start"},children:Object(a.jsx)(b,{children:P})}),Object(a.jsx)("div",{className:"text-row",children:Object(a.jsx)("div",{className:"content-image",style:{backgroundImage:"url(".concat(L,")"),position:"relative"}})})]}),Object(a.jsxs)("div",{className:"block-container",children:[Object(a.jsx)("div",{className:"text-row",style:{justifyContent:"start"},children:Object(a.jsx)(b,{children:S})}),Object(a.jsx)("div",{className:"result-image",style:{backgroundImage:"url(".concat(_,")"),position:"relative"}}),Object(a.jsx)("div",{className:"result-image",style:{backgroundImage:"url(".concat(E,")"),position:"relative"}}),Object(a.jsx)("div",{className:"result-image",style:{backgroundImage:"url(".concat(F,")"),position:"relative",paddingBottom:"44%"}})]})]})};var S=function(){return Object(a.jsx)("div",{className:"App",children:Object(a.jsx)(P,{})})},V=function(e){e&&e instanceof Function&&n.e(3).then(n.bind(null,23)).then((function(t){var n=t.getCLS,a=t.getFID,i=t.getFCP,s=t.getLCP,r=t.getTTFB;n(e),a(e),i(e),s(e),r(e)}))};o.a.render(Object(a.jsx)(s.a.StrictMode,{children:Object(a.jsx)(S,{})}),document.getElementById("root")),V()}},[[22,1,2]]]);
//# sourceMappingURL=main.64522adc.chunk.js.map